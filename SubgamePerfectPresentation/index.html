<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title> An Algebraic View of Convex Optimization </title>

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="reveal.css">
		<link rel="stylesheet" href="solarized.css" id="theme">
        <style>
            p {font-size: 30px}
            li {font-size: 30px}
            .path {
            stroke-dasharray: 1000;
            stroke-dashoffset: 1000;
            animation: dash 2s linear forwards;
            }

            @keyframes dash {
            to {
                stroke-dashoffset: 0;
            }
            }

            tr {
                border-style:hidden;
            }
            
        </style>
	</head>

	<body>

		<div class="reveal">

			<div class="slides">

				<section>
                    <section>
                        <h2> An Algebraic View of Convex Optimization </h2>
                        <p>Kevin Shu</p>
                        <p style="font-size:0.75em">California Institute of Technology</p>
                    </section>
                    <section>
                        <div style="display:flex">
                            <div style="flex:1">
                                <img src="ben.jpg" height="200px"></img>
                                <p> <b>Ben Grimmer</b> - Johns Hopkins University</p>
                            </div>
                            <div style="flex:1">
                                <img src="alex.jpeg" height="200px"></img>
                                <p> <b>Alex L. Wang</b> - Purdue University</p>
                            </div>
                    </section>
				</section>

                <section>
                    <section data-auto-animate>
                        <h3>Introduction and History</h3>
                        <div style="text-align:left; padding:0.35in; background:#EEEEEE">
                        <h5> Definition </h5>
                        <p style="font-size:0.75em">We say that $f:\R^d \rightarrow \R$ is convex if whenever $x, y \in \R^d$ and $t \in [0,1]$,
                        \[f(t x + (1-t) y) \le t f(x) + (1-t)f(y).\]
                        </p>
                        <p style="font-size:0.75em">We say that $f$ is $L$<i>-smooth</i> if it is differentiable everywhere and its gradient is $L$-Lipschitz, i.e.
                        \[\|\nabla f(x) - \nabla f(y)\| \le L \|x-y\|.\]
                        </p>
                        </div>
                        <p class="fragment"><b>Given an oracle for evaluating and differentiating an $L$-smooth convex $f$, we want to find an approximate minimum of $f$.</b></p>
                    </section>
                    <section data-auto-animate>
                        <h4>Introduction and History</h4>
                        <p>
                            Basic idea: any first order method can only see the function and its gradients at a finite number of points.
                        </p>
                        <img src="3d_surface.svg"/>
                    </section>
                    <section data-auto-animate>
                        <h4>Introduction and History</h4>
                        <p>
                            Basic idea: any first order method can only see the function and its gradients at a finite number of points.
                        </p>
                        <img src="3d_surface_with_points.svg"/>
                    </section>
                    <section data-auto-animate>
                        <h4>Introduction and History</h4>
                        <p>
                            Basic idea: any first order method can only see the function and its gradients at a finite number of points.
                        </p>
                        <div style="text-align:left; padding:0.35in; background:#EEEEEE">
                        <h4> Definition </h4>
                        <p> Given a set of triples $\mathcal{T} = \{(x_i, f_i, g_i)\}_{i=0}^n \subseteq \R^d \times \R \times \R^d$, we say that $\mathcal{T}$ is <i>interpolable</i> if there exists an $L$-smooth convex function $f$ so that $f(x_i) = f_i$ and $\nabla f(x_i) = g_i$ for each $i$.</p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4>Introduction and History</h4>
                        <div style="text-align:left; padding:0.35in; font-size:0.75em; background:#EEEEFF">
                            <h4> Theorem (Taylor, Hendrickx, Glineur 2016)</h4>
                            <p>
                            $\mathcal{T}$ is interpolable if and only if for each $i, j \in \{0, \dots, n\}$,
                            \[
                                Q_{ij} = f_i - f_j - \langle g_j, x_i - x_j \rangle - \frac{1}{2L}\|g_i-g_j\|^2
                            \]
                            </p>
                        </div>
                        <p> It turns out that this implies that the problem of determining the convergence rate for a fixed number of steps of many algorithms reduces to an SDP.</p>
                    </section>
                    <section data-auto-animate>
                        <h4>Introduction and History</h4>
                        <p>
                            An iterative algorithm defines a sequence of query points $x_0, x_1, \dots, x_N$, where $x_i$ depends on the values of the function and its gradients at $x_0, \dots, x_{i-1}$.
                        </p>
                        <p>
                            <b>Want a bound</b>
                            \[
                            f(x_N) - f(x_*) \le \frac{L}{2\tau} \|x_0-x_*\|^2,
                            \]
                            where $x_*$ is a minimizer and $\tau$ is as large as possible.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Performance Estimation (Sums of Squares)</h4>
                        <p>
                        Want to show that for any $L$-smooth convex function $f$ with minimizer $x_*$, 
                        \[
                             P = \frac{L}{2\tau} \|x_0-x_*\|^2 - (f(x_N) - f(x_*)) \ge 0.
                        \]
                        </p>
                        <p>
                            This is a polynomial we want to be nonnegative on a semialgebraic set: apply sums of squares techniques.
                        </p>
                        <p class="fragment">
                            Find a certificate of the form
                            \[
                                P = \sum_{i,j} \lambda_{ij}Q_{ij} + S,
                            \]
                            where $\lambda_{ij} \ge 0$, and $S$ is something positive semidefinite.
                        </p>
                        <p class="fragment">
                        This is equivalent to the performance estimation problem methodology popularized by Drori and Teboulle in 2012.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Performance Estimation </h4>
                        <p>
                            <b>First try:</b>
                            For each $i,j \in [n] \cup \{*\}$, find  $\lambda_{ij} \ge 0$ so that
                            \[
                                \frac{L}{2} \|x_0-x_*\|^2 - \tau(f(x_N) - f(x_*)) = \sum_{i, j \in [n] \cup \{*\}} \lambda_{ij} Q_{ij} \ge 0.
                            \]
                            Rearranging would imply the bound we want for the final suboptimality.
                        </p>
                        <p class="fragment">
                            Issue: this is impossible;
                            \[
                                Q_{ij} = f_i - f_j - \langle g_j, x_i - x_j \rangle - \frac{1}{2L}\|g_i-g_j\|^2
                            \]
                            does not involve any terms of the form $\|x_*\|^2$.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Performance Estimation </h4>
                        <p>
                            <b>Modified Proof idea:</b>
                            For each $i,j \in [n] \cup \{*\}$, find  $\lambda_{ij} \ge 0$ so that
                            \[
                            \frac{L}{2} (\|x_0-x_*\|^2\color{red}{ - \|z_N - x_*\|^2}) - \tau(f(x_N) - f(x_*)) = \sum_{i, j \in [n] \cup \{*\}} \lambda_{ij} Q_{ij},
                            \]
                            where $z_N \in \R^d$ is arbitrary.
                        </p>
                        <p class="fragment">
                        This introduces a "slack" term $\color{red}{\|z_N - x_*\|^2}$, and this turns out to the minimal slack needed for such a proof to work.
                        </p>
                        <p class="fragment">
                            This $z_N$ appears in many algorithms as a `momentum' variable.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Performance Estimation </h4>
                        <p>
                            <b>Example (gradient descent):</b>
                            Take $N = 1$ and $x_1 = x_0 - \frac{1}{L}\nabla f(x_0)$. Set $g_0 = \nabla f(x_0)$ and $g_1 = \nabla f(x_1)$.
                        </p>
                        <p class="fragment" style="font-size:0.75em">
                            \[
                                Q_{*0} = f_* - f_0 - \langle g_0, x_* - x_0\rangle - \frac{1}{2L} \|g_0\|^2 \ge 0
                            \]
                            \[
                                Q_{*1} = f_* - f_1 - \langle g_1, x_* - x_1\rangle - \frac{1}{2L} \|g_1\|^2 \ge 0
                            \]
                            \[
                                Q_{01} = f_0 - f_1 - \langle g_1, x_0 - x_1\rangle - \frac{1}{2L} \|g_0 - g_1\|^2 \ge 0
                            \]
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Performance Estimation </h4>
                        <p>
                            <b>Example (gradient descent):</b>
                            Take $N = 1$ and $x_1 = x_0 - \frac{1}{L}g_0$, where $g_0 = \nabla f(x_0)$ and $g_1 = \nabla f(x_1)$.
                        </p>
                        <p style="font-size:0.75em">
                            \[
                                Q_{*0} = f_* - f_0 - \langle g_0, x_* - x_0\rangle - \frac{1}{2L} \|g_0\|^2 \ge 0
                            \]
                            \[
                                Q_{*1} = f_* - f_1 - \langle g_1, x_* - \color{red}{(x_0-\frac{1}{L}g_0)}\rangle - \frac{1}{2L} \|g_1\|^2 \ge 0
                            \]
                            \[
                                Q_{01} = f_0 - f_1 - \langle g_1, x_0 -  \color{red}{(x_0-\frac{1}{L}g_0)}\rangle - \frac{1}{2L} \|g_0 - g_1\|^2 \ge 0
                            \]
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Performance Estimation </h4>
                        <p style="font-size:0.75em">
                            Taking $\tau = 1$ and $z_1 = x_0  - \frac{1}{L}(g_0 + g_1)$ yields
                            \[
                                \frac{1}{2}(Q_{*0} + Q_{*1} + Q_{01}) = 
                                f_* - f_1 +
                                \frac{L}{2}(\|x_0 - x_*\|^2 - \|z_1 - x_*\|^2)
                                 \]
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Momentum: a first taste </h4>
                        <p> Suppose that we have taken $n-1$ steps in some algorithm, and can certify that 
                        \[
                            H_{n-1} = \tau_{n-1}(f_* - f_{n-1}) + \frac{L}{2}(\|x_0 - x_*\|^2 - \|z_{n-1} - x_*\|^2) \ge 0
                        \]
                        </p>
                        <p class="fragment">
                        Can we use this inequality <b>inductively</b> to get an inequality of the form
                        \[
                            H_n = \tau_n(f_* - f_{n}) + \frac{L}{2}(\|x_0 - x_*\|^2 - \|z_{n} - x_*\|^2)?
                        \]
                        </p>
                        <p class="fragment">
                        Specifically, can we <b> choose </b> <span style="color:red">$x_n$</span>, <span style="color:red">$z_n$</span>, and <span style="color:blue">$\tau_n$</span> so that $H_n$ is a weighted sum of $H_{n-1}$ and the $Q_{ij}$'s?
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Momentum: a first taste </h4>
                        <p><b> A key simplification: </b></p>
                        <p> Not all of the $Q_{ij}'s$ are useful.</p>
                        <p class="fragment"> 
                            At iteration $n-1$, all of the quantities $f_0, \dots, f_{n-1}$ and $g_0, \dots, g_{n-1}$ are known and fixed. Thus, we can treat these things as constants. In particular, this implies that $Q_{ij}$ for $i, j \le n-1$ are also constants.
                        </p>
                        <p class="fragment">
                            Also, the inequalities
                            \[
                                Q_{i*} = f_i - f_* - \frac{1}{2L}\|g_i\|^2 \ge 0
                            \]
                            \[
                                Q_{ni} = f_n - f_i \dots \ge 0
                            \]
                            `go the wrong way: they are either upper bounds on $f_*$ or lower bounds on $f_n$.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Momentum: a first taste </h4>
                        <p>
                            In fact, in this setting, we can restrict our attention to just 3 inequalities:
                            \[ Q_{n-1, n}, Q_{* n}, Q_{*n-1} \ge 0.\]
                        </p>
                        <p class="fragment">
                            No matter what $f_n, g_n, f_*, x_*$ are, we want
                            \[
                                H_n = \mu H_{n-1} + \lambda_{n-1 n}Q_{* n} + \lambda_{* n}Q_{* n} +  \lambda_{* n-1}Q_{* n-1}.
                            \]
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Momentum: a first taste </h4>
                        <p>
                            No matter what $f_n, g_n, f_*, x_*$ are, we want
                            \[
                                H_n = \mu H_{n-1} + \lambda_{n-1 n}Q_{* n} + \lambda_{* n}Q_{* n} +  \lambda_{* n-1}Q_{* n-1}.
                            \]
                        </p>
                        <p class="fragment">
                            Each term in this expression should be thought of as a polynomial in the unknown expressions 
                            $f_n, g_n, f_*, x_*$ (where the coefficient of $\|x_*\|^2$ is 0):

                            \[
                                C + a f_n + b f_* + \langle v, g_n\rangle + \langle w, x_*\rangle + c \|g_n\|^2 + d \langle g_n, x_*\rangle.
                            \]
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Momentum: a first taste </h4>
                        <p>
                            Each term in this expression should be thought of as a polynomial in the unknown expressions 
                            $f_n, g_n, f_*, x_*$:

                            \[
                            \color{blue}C + \color{blue}a f_n + \color{blue}b f_* + \langle \color{red}v, g_n\rangle + \langle \color{red}w, x_*\rangle + \color{blue}c \|g_n\|^2 + \color{blue}d \langle g_n, x_*\rangle.
                            \]
                        </p>
                        <p>
                        There are 5 <span style="color:blue">scalar</span> coefficients and 2 <span style="color:red">vector</span> coefficients in this expression, so the equation
                        \[
                            H_n = \mu H_{n-1} + \lambda_{n-1 n}Q_{* n} + \lambda_{* n}Q_{* n} +  \lambda_{* n-1}Q_{* n-1}.
                        \]
                        amounts to 5 scalar equations and 2 vector equations.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Momentum: a first taste </h4>
                        <p>
                            There are 6 <span style="color:blue">scalar</span> coefficients and 2 <span style="color:red">vector</span> coefficients in this expression, so the equation
                            \[
                                H_n = \mu H_{n-1} + \lambda_{n-1 n}Q_{* n} + \lambda_{* n}Q_{* n} +  \lambda_{* n-1}Q_{* n-1}.
                            \]
                            amounts to 5 scalar equations and 2 vector equations.
                        </p>
                        <p>

                            We are allowed to make choices for <span style="color:blue">$\tau$, $\mu$, $\lambda_{n-1 n}$, $\lambda_{* n}$, $\lambda_{* n-1}$</span> and 
                            <span style="color:red">$x_n$, $z'$</span>, so the number of equations is equal to the number of degrees of freedom. We would expect a <b>finite number of </b> solutions that allows for this equation to hold.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Momentum: a first taste </h4>
                        <img src="mathematica_example.png"/>
                    </section>
                    <section data-auto-animate>
                        <h4> Momentum: a first taste </h4>
                        <p>
                             \[\tau_n = \frac{1 + \sqrt{1 + 4(1-\delta) + 4 \tau_{n-1} + 
                             4 \sqrt{(1-\delta)^2 + \theta_{n-1}^2}}}{2}\]
                             \[x_{n} = z_{n-1} + \alpha (x_{n-1} - z_{n-1}) - \beta g_{n-1}\]
                             \[z_{n} = z_{n-1} - \sqrt{\tau_n}(\beta-\alpha)g_{n-1}-\sqrt{\tau_n} g_n,\]
                             where $\delta, \alpha, \beta$ are some expressions in terms of $\tau_n$.
                        </p>
                        <p>This turns out to be a variant of the Nesterov fast gradient method, which was the first method to achieve a convergence rate of $\tau_n = \Omega(n^2)$ (this is the best possible up to constant factors).</p>
                        <p class="fragment">Can we do better?</p>
                    </section>
                    <section data-auto-animate>
                        <h4> Momentum: Optimized </h4>
                        <p>
                        Modify the inductive hypothesis a little
                        \[
                            H_{n-1} = \tau_{n-1}(f_* - f_{n-1} \color{red}{+ \frac{1}{2}\|g_{n-1}\|^2}) + \frac{L}{2}(\|x_0 - x_*\|^2 - \|z_{n-1} - x_*\|^2) \ge 0
                        \]
                        </p>
                        <p>
                        Applying the same methodology as above leads to the optimized gradient method (OGM) found by Kim and Fessler in 2016.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Momentum: Optimized </h4>
                        <p>
                        OGM is optimal in the following (minimax sense):
                        </p>
                        <div style="text-align:left; padding:0.35in; font-size:0.75em; background:#EEEEFF">
                            <h4> Theorem (Drori 2017)</h4>
                            <p>
                            There exists a function $f_{hard} :\R^{N+2} \rightarrow \R$ and $x_0 \in \R^{N+2}$ so that any iterative algorithm with the property that
                            \[
                                x_{i+1} \in x_0 + \text{span}(\nabla f_{hard}(x_0), \dots, \nabla f_{hard}(x_i)),
                            \]
                            \[
                                f(x_N) - f_* \ge \frac{L}{2\tau_N} \|x_0 -x_*\|^2,
                            \]
                            where $\tau_N$ is defined by the OGM recurrence.
                            </p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4> Momentum: Beyond Worst Case </h4>
                        <p>OGM converges at the same rate for these functions:</p>
                        <div style="display:flex">
                            <div style="flex:1">
                                <img src="complicated.png" height="300px"/>
                                <p>The Drori hard function </p>
                            </div>
                            <div style="flex:1">
                                <img src="simple.gif" height="300px"/>
                                <p>$f(x) = \frac{1}{2}x^2$</p>
                            </div>
                        </div>
                        <p class="fragment">
                        After 2 gradient queries, the minimizer of $f(x) = \frac{1}{2}x^2$ is uniquely determined.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Momentum: Beyond Worst Case </h4>
                        <p>The issue is that OGM only considers the current gradient when making its update; earlier gradient queries can be useful when the function is nonadversarially chosen.</p>
                        <p class="fragment">What if we tried a more complicated induction of the form
                        \[
                            H_n = \sum_{i=0}^{n-1}\mu_i H_{i} + \sum_{i=0}^{n}\lambda_{* n}Q_{* n} +  \sum_{i=0}^{n-1}\lambda_{i n}Q_{i n}?
                        \]
                        </p>
                        <p class="fragment">This leads to an algorithm we call the subgame perfect gradient method.</p>
                    </section>
                    <section data-auto-animate>
                        <h4> Momentum: Subgame Perfect Gradient Method </h4>
                        <p> We found a first order method with the following <i>dynamic</i> guarantee: </p>
                        <div style="text-align:left; padding:0.35in; font-size:0.75em; background:#EEEEFF">
                            <h4> Theorem (Grimmer, S, Wang) </h4>
                            <p>
                            The algorithm (SPGM) provides the following guarantee: if $\mathcal{H} = \{(x_i, f_i, g_i)\}_{i=0}^n$ were the first order information produced by $n$ iterations of SPGM, then there exists some $\hat{\tau}_N$ (depending on $\mathcal{H}$) so that for any function $f$ so that $f(x_i) = f_i$ and $\nabla f(x_i) = g_i$,
                            \[
                                f(x_N) - f(x_*) \le \frac{L}{2\hat{\tau}_N} \|x_0-x_*\|^2.
                            \]
                            </p>
                        </div>
                    </section>
                    <section  data-auto-animate>
                        <h4> Momentum: Subgame Perfect Gradient Method </h4>
                        <p> We found a first order method with the following <i>dynamic</i> guarantee: </p>
                        <div style="text-align:left; padding:0.35in; font-size:0.75em; background:#EEEEFF">
                            <h4> Theorem (Cont) </h4>
                            <p>
                            On the other hand, there exists a function $f_{\mathcal{H}}$ agreeing with the history $\mathcal{H}$ and so that any sequence $x_{n+1}, \dots, x_{N}$ satisfying
                            \[
                            x_i \in x_0 + \text{span} \{\nabla f(x_0), \dots, \nabla f(x_{i-1})\},
                            \]
                            \[
                                f_{\mathcal{H}}(x_N) - f_{\mathcal{H}}(x_*) \ge \frac{L}{2\hat{\tau}_N} \|x_0-x_*\|^2.
                            \]
                            </p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h3> Acceleration with Gradient Descent </h3>
                        <p><b> Can we get acceleration with methods of the form
                        \[
                            x_{i+1} = x_i - h_i \nabla f(x_i)?
                        \]
                        </b>
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h3> Acceleration with Gradient Descent </h3>
                        <h4> History </h4>
                        <ul>
                            <li> Vanilla gradient descent ($x_{i+1} = x_i - \frac{1}{L}\nabla f(x_i)$): converges at a rate of $\frac{1}{2n}$ </li>
                            <li> Vanilla gradient descent: analysis improved to $\frac{1}{4n}$ in Drori and Teboulle 2012, also showing that any constant step size between $0$ and $2$ yield similar convergence. </li>
                            <li> Existence of step size choices leading to convergence at a rate of $O(\frac{1}{n^{1+\epsilon}})$ was shown simultaneously by (Altschuler and Parrilo) and (Grimmer, S, Wang) in 2023. </li>
                        <ul>
                    </section>
                    <section data-auto-animate>
                        <h3> Acceleration with Gradient Descent </h3>
                        <h4> History </h4>
                        <img src="silver.png" />
                        <p>
                            \[
                            h_i = 1+\rho^{\nu(i) - 1},
                            \]
                            where $\rho = 1+\sqrt{2}$ and $\nu(i)$ is the largest power of 2 dividing $i$.
                        </p>
                        <p class="fragment">
                            This leads to a convergence rate of $O(\frac{1}{n^{\log_2(\rho)}}$.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h3> Acceleration with Gradient Descent </h3>
                        <p> In fact, the silver step sizes are one of a large family of step size patterns that are obtainable via a recursive procedure, which were discovered simultaneously in (Grimmer, S, Wang 2024) and (Jiang, Zhang 2024). </p>
                        <p> Loosely, we find that there are `composable' step size sequences, so that if $\alpha$ and $\beta$ are composable step size sequences, then there exists a $\mu \in \R$ so that the composition
                        \[
                            \alpha \join \beta = [\alpha, \mu, \beta]
                        \]
                        is composable with an improved rate of convergence.
                        </p>
                        <p class="fragment">
                             Here, $\mu$ solves some degree $2$ polynomial equation in order to make the composition $\alpha \join \beta$ have the same performance on the quadratic function $f(x) = \frac{1}{2}x^2$ and the Huber function $f(x) = \begin{cases}|x|+\frac{1}{2} \text{ if x > \frac{1}{2}}\\x^2$.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h3> Acceleration with Gradient Descent </h3>
                        <div style="text-align:left; padding:0.35in; font-size:0.75em; background:#EEEEFF">
                        <h4> Theorem </h4>
                            <p>
                            There are composable sequences with rate $r \le \frac{0.42311}{(n+1)^{1.27\dots}},$ where
                            $1.27\dots = \log_2(1+\sqrt{2})$.
                            </p>
                        </div>
                        <ul class=fragment>
                            <li> Best known constant factor for gradient descent methods.</li>
                            <li> We find sequences that match or exceed the performance of the step size sequences found by branch-and-bound search in Das Gupta et al. for each $n$ that they considered.</li>
                            <li> These sequences can be obtained by repeated composing the empty sequence with itself.</li>
                        </ul>
                    </section>
                    <section data-auto-animate>
                        <h3> Questions for Algebraists </h3>
                        <ul>
                            <li>Why do these problems have such nice solutions (solvable by radicals, large degeneracy, Grobner bases generated by quadrics, etc)? </li>
                            <li>Why are worst case functions often 1 dimensional (why do the dual SDPs to these performance estimation problems have rank 1 solutions)? </li>
                            <li>Can we show lower bounds that there are no solutions to these problems with rates beating the $O(\frac{1}{n^{\log_2(\rho)}})$ convergence rates? </li>
                        </ul>
                    </section>
            </section> </div>

		</div>

		<script src="reveal.js"></script>
		<script src="math.js"></script>
		<script>
			Reveal.initialize({
				history: true,
				transition: 'linear',

				mathjax2: {
					config: 'TeX-AMS_HTML-full',
					TeX: {
						Macros: {
							R: '\\mathbb{R}',
							E: '\\mathbb{E}',
							set: [ '\\left\\{#1 \\; ; \\; #2\\right\\}', 2 ]
						}
					}
				},

				// There are three typesetters available
				// RevealMath.MathJax2 (default)
				// RevealMath.MathJax3
				// RevealMath.KaTeX
				//
				// More info at https://revealjs.com/math/
				plugins: [ RevealMath.MathJax2 ]
			});
            Reveal.addEventListener( 'drawCurve', function() {
                element = document.getElementById("curveDrawing");
                element.classList.add("path");
            } );
		</script>

	</body>
</html>
