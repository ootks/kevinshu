<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title> Gradient Descent: An Algebraic Perspective </title>

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="reveal.css">
		<link rel="stylesheet" href="solarized.css" id="theme">
        <style>
            p {font-size: 30px}
            li {font-size: 30px}
            .path {
            stroke-dasharray: 1000;
            stroke-dashoffset: 1000;
            animation: dash 2s linear forwards;
            }

            @keyframes dash {
            to {
                stroke-dashoffset: 0;
            }
            }

            tr {
                border-style:hidden;
            }
            
        </style>
	</head>

	<body>

		<div class="reveal">

			<div class="slides">

				<section>
                    <section>
                        <h2> Novel First-Order Methods via Performance Estimation Programming</h2>
                        <p>Kevin Shu</p>
                        <p style="font-size:0.75em">California Institute of Technology</p>
                    </section>
                    <section>
                        <div style="display:flex">
                            <div style="flex:1">
                                <img src="ben.jpg" height="200px"></img>
                                <p> <b>Ben Grimmer</b> - Johns Hopkins University</p>
                            </div>
                            <div style="flex:1">
                                <img src="alex.jpeg" height="200px"></img>
                                <p> <b>Alex L. Wang</b> - Purdue University</p>
                            </div>
                    </section>
                    <section>
                        <h3> Outline </h3>
                        <ul>
                            <li> Introduction and History of First Order Methods</li>
                            <li> Long steps and our contributions </li>
                            <li> Composition of step size sequences </li>
                            <li> Recursive certificates </li>
                        </ul>
                    </section>
				</section>

                <section>
                    <section data-auto-animate>
                        <h3>Introduction and History</h3>
                        <div style="text-align:left; padding:0.35in; font-size:0.75em; background:#EEEEEE">
                        <h4> Definition </h4>
                        <p>We say that $f$ is $L$<i>-smooth</i> if it is differentiable everywhere and its aradient is $L$-Lipschitz, so that for any $x, y \in \R^d$,</p>
                        \[\|\nabla f(x) - \nabla f(y)\| \le L \|x-y\|.\]
                        </div>
                        <p><b>Given an oracle for evaluating and differentiating an $L$-smooth convex $f$, we want to find an approximate minimum of $f$.</b></p>
                        <p class = fragment>
                            Gradient descent repeatedly iterates
                            \[
                                x_{i+1} = x_i - \frac{h_i}{L} \nabla f(x_i).
                            \]
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h3>Introduction and History</h3>
                        <p><b>Given an oracle for evaluating and differentiating $f$, we want to find an approximate minimum of $f$.</b></p>

                        <h4> History </h4>
                        <div style="text-align:left; font-size:0.25em ">
                        <ul>
                            <li style="font-size:2em"> <b>Standard: </b> If each $h_i = 1$ in gradient descent, \[f(x_n) - f(x_*) \le \frac{L}{2n} \|x_0 - x_*\|^2.\]</li>
                            <li style="font-size:2em"> <b>Fast Gradient Method (Nesterov):</b> There is a gradient method (using momentum) so that, \[f(x_n) - f(x_*) \le \frac{2L}{(n+1)^2} \|x_0 - x_*\|^2.\]</li>
                            <li style="font-size:2em"> <b>Performance estimation (Drori and Teboulle):</b> If each $h_i = h$ with $0 &lt; h \le 1$ in gradient descent, \[f(x_n) - f(x_*) \le \frac{L}{4 n h + 2} \|x_0 - x_*\|^2.\]</li>
                            <li style="font-size:2em"> <b>Optimized Gradient Method (Kim and Fessler):</b> Gives an explicit gradient method that achieves the best possible convergence rate (even up to constants).
                        </ul>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h3>Introduction and History</h3>
                        <p><b>Given an oracle for evaluating and differentiating $f$, we want to find an approximate minimum of $f$.</b></p>

                        <h4> History </h4>
                        <div style="text-align:left; font-size:0.25em ">
                        <ul>
                            <li style="font-size:2em; color:red"> <b>Performance estimation (Drori and Teboulle):</b> If each $h_i = h$ with 0 &lt; h \le 1$ in gradient descent, \[f(x_n) - f(x_*) \le \frac{L}{4 n h + 2} \|x_0 - x_*\|^2.\]</li>
                            <li style="font-size:2em; color:red"> <b>Optimized Gradient Method (Kim and Fessler):</b> Gives an explicit gradient method that achieves the best possible convergence rate (even up to constants).
                        </ul>
                        </div>
                        <p> The last two bounds use the Performance Estimation Problem (PEP) framework to prove these bounds. </p>
                    </section>
                    <section data-auto-animate>
                        <h3>Introduction and History</h3>
                        <p> Classic analysis for gradient descent concerned step sizes between 0 and 2 (short steps), for which $O\left(\frac{1}{n}\right)$ rates of convergence are best possible </p>
                        <p class=fragment> Subsequent work (Altschuler; Das Gupta, Van Parys, Ryu; Grimmer) pushed the analysis beyond the short step regime, but could only handle sequences of a fixed size.</p>
                        <p class=fragment> Grimmer, S. and Wang, and concurrently Altschuler and Parrilo found sequences of unbounded length that obtained rates of $O\left(\frac{1}{n^p}\right)$, where $p &gt; 1$. </p>
                        <p class=fragment> <b> We will describe the methods by which these convergence rates are proven, and some families of step sizes that produce various guarantees. </b></p>
                    </section>
                    <section data-auto-animate>
                        <h3> Acceleration with Gradient Descent </h3>
                        <p><b> Why is acceleration possible in this setting? </b>

                        <p> Two extreme functions: </p>
                        <div style="display:flex">
                            <div style="flex:1">
                                <img  height=200px src="quadratic.png"/>
                                <p style="font-size:0.5em">$f(x) = \frac{1}{2}x^2$</p>
                            </div>
                            <div style="flex:1">
                                <img height=200px src="linear.png"/>
                                <p style="font-size:0.5em">$f(x) = x$</p>
                            </div>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h3> Acceleration with Gradient Descent </h3>
                        <p><b> Why is acceleration possible in this setting? </b>

                        <p> Two extreme functions: </p>
                        <div style="display:flex">
                            <div style="flex:1">
                                <img  height=200px src="quadratic.png"/>
                                <p style="font-size:0.5em">$f(x) = \frac{1}{2}x^2$</p>
                            </div>
                            <div style="flex:1">
                                <img height=200px src="huber.png"/>
                                <p style="font-size:0.3em;margin-top:-10px">$f(x) = \begin{cases}|x| \text{ if }|x| < 1\\\\ \frac{1}{2}x^2\text{ otherwise}\end{cases}$</p>
                            </div>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h3> Acceleration with Gradient Descent </h3>
                        <p><b> Why is acceleration possible in this setting? </b>

                        <p> Two extreme functions: </p>
                        <div style="display:flex">
                            <div style="flex:1">
                                <img  height=200px src="ezgif-2-cd40e6ea8b.gif"/>
                                <p style="font-size:0.5em">$f(x) = \frac{1}{2}x^2$</p>
                                <p class=fragment style="font-size:0.5em">Large steps lead to divergence.</p>
                            </div>
                            <div style="flex:1">
                                <img height=200px src="huber.gif"/>
                                <p style="font-size:0.3em;margin-top:-10px">$f(x) = \begin{cases}|x| \text{ if }|x| < 1\\\\ \frac{1}{2}x^2\text{ otherwise}\end{cases}$</p>
                                <p  class=fragment style="font-size:0.5em">Small steps converge slowly.</p>
                            </div>
                        </div>
                        <p class=fragment style="margin-top:-20px"> <b> Alternating big and small steps leads to faster convergence for both. </b> </p>
                    </section>
                    <section data-auto-animate>
                        <h3> Acceleration with Gradient Descent </h3>
                        <p> These two functions (quadratic and Huber) appear to be more than just illustrative: </p>
                        <ul>
                            <li>They are both worst case functions for the optimized gradient method. </li>
                            <li>They are (conjectured to be) worst case for the best constant step size gradient descent method. </li>
                            <li>We will be designing gradient descent steps for which the worst case rate is obtained by both of these functions. </li>
                        </ul>
                    </section>
                    <section data-auto-animate>
                        <h3> Recursive Construction of Step Sizes </h3>
                        <p> The step sizes we construct are <b> recursively </b> designed to <b> have Huber and quadratic functions as their worst case.</b></p>
                        <p class=fragment>
                        Roughly, we will define a class of sequences that are `composable' so that if $h$ and $\ell$ are composable, then there is some $\mu \in \R$ so that
                        \[
                            [h, \mu, \ell]
                        \]
                        is still composable, and have an improved rate of convergence over either of the constituents.
                        </p>
                        <p class=fragment> <b> Slight complication: </b> We will actually need multiple `flavors' of composable sequences, that need to be composed slightly differently. </p>
                    </section>
                    <section data-auto-animate>
                        <h3> $f$-Composable Sequences</h3>
                        <div style="text-align:left; padding:0.35in; font-size:0.75em; background:#EEEEEE">
                        <h4> Definition </h4>
                            <p>
                            We will say that a sequence $h \in \R^n$ is <i>$f$-composable</i> with rate $r$ if for any smooth convex function $f$, if $x_0, \dots, x_n$ are the iterates of gradient descent with step sizes $h$,
                            \[
                                f(x_n) - f(x_*) \le \frac{r}{2}\|x_0 - x_*\|^2,
                            \]
                            and that this inequality is met with equality when $f$ is either the Huber function or the 1-d quadratic function $\frac{1}{2}x^2$.
                            </p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h3> $f$-Composable Sequences</h3>
                        <p><b>How do construct such $f$-composable sequences?</b></p>
                        <p class=fragment> If $h_1, h_2$ are two sequences, there is a unique value of $\mu&gt;1$ so that the concatenation
                        \[
                            [h_1, \mu, h_2]
                        \]
                        performs equally on the Huber and quadratic function.
                        </p>
                        <p class=fragment> <b>Composition of two $f$-composable sequences is not necessarily $f$-composable.</b></p>
                    </section>
                    <section data-auto-animate>
                        <h3> $s$-Composable Sequences</h3>
                        <p>There is another of composable sequence that we consider. The definition is slightly more complicated than $f$-composable, so we will only mention the important theorem:</p>
                        <div style="text-align:left; padding:0.35in; font-size:0.75em; background:#EEEEFF">
                            <h4> Theorem </h4>
                            <p style="font-size:0.75em">
                            If $h$ and $\ell$ are $s$-composable sequences with rates $r_1$ and $r_2$ respectively, then there is a unique constant $\mu &gt; 1$ so that 
                                \[
                                    [h, \mu, \ell]
                                \]
                                is $s$-composable with rate $\frac{2r_1r_2}{r_1 + r_2 + \sqrt{r_1^2 + 6r_1r_2 + r_2^2}}$. Denote this by $h \Join \ell$.
                            </p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h3> $s$-Composable Sequences</h3>
                        <p> If we start with the $s$ composable sequence with no entries (which has rate 1), and repeatedly compose with itself, we get the sequence
                        \[
                        \sigma^1 = [] \Join [] = [\sqrt{2}]
                        \]
                        \[
                            \sigma^2 = (\sigma^1)\Join(\sigma^1)  = 
                            [\sqrt{2}, 2 ,\sqrt{2}]
                        \]
                        \[
                            \sigma^3 = (\sigma^2)\Join(\sigma^2) = [\sqrt{2}, 2 ,\sqrt{2}, 2+\sqrt{2}, \sqrt{2}, 2 ,\sqrt{2}, ]
                        \]
                        $\dots$
                        </p>
                        <p> This is the silver step size sequence defined by Altschuler and Parrilo.  We also get sequences like
                        \[
                            \sigma^1 \Join \sigma^2 = [\sqrt{2}, \frac{2 \left(\sqrt{2}-1\right)}{\left(\sqrt{2}+2\right) \left(\sqrt{2 \sqrt{2}-1}+1\right)}, \sqrt{2}, 2, \sqrt{2}].
                        \]
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h3> $s$-Composable Sequences</h3>
                        <p>We can also compose $s$-composable sequences with $f$-composable sequences.</p>
                        <div style="flex:1;margin:0.25in;text-align:left; padding:0.35in; font-size:0.75em; background:#EEEEFF">
                        <h4> Theorem </h4>
                            <p style="font-size:0.75em">
                                If $h$ is $s$-composable with rate $r_1$ and $\ell$ is $f$-composable with rate $r_2$, then there is a constant $\mu$ so that
                                \[
                                    [h, \mu, \ell]
                                \]
                                is $f$-composable with rate $\frac{2r_1r_2}{r_1 + 4r_2 + \sqrt{r_1^2+8r_1r_2}}$.
                            </p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h3> $f$-Composable Sequences</h3>
                        <div style="text-align:left; padding:0.35in; font-size:0.75em; background:#EEEEFF">
                        <h4> Theorem </h4>
                            <p>
                            There are $f$-composable sequences with rate $r \le \frac{0.433}{(n+1)^{1.27\dots}},$ where
                            $1.27\dots = \log_2(1+\sqrt{2})$.
                            </p>
                        </div>
                        <ul class=fragment>
                            <li> Best known constant factor for gradient descent methods.</li>
                            <li> We find sequences that match or exceed the performance of the step size sequences found by branch-and-bound search in Das Gupta et al. for each $n$ that they considered.</li>
                        </ul>
                    </section>

                    <section data-auto-animate>
                        <h3> $g$-Composable Sequences</h3>
                        <div style="text-align:left; padding:0.35in; font-size:0.75em; background:#EEEEEE">
                        <h4> Definition </h4>
                            <p>
                                We will say that a sequence $h \in \R^n$ is <i>$g$-composable</i> with rate $r$ if for any smooth convex function $f$, if $x_0, \dots, x_n$ are the iterates of gradient descent with step sizes $h$,
                                \[
                                    \|\nabla f(x_n)\|^2 \le r(f(x_0) - f(x_*))
                                \]
                                and that this inequality is met with equality when $f$ is either the Huber function or the 1-d quadratic function $\frac{1}{2}x^2$.
                            </p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h3> $g$-Composable Sequences</h3>
                        <div style="text-align:left; padding:0.35in; font-size:0.75em; background:#EEEEFF">
                        <h4> Theorem </h4>
                            <p>
                            There are $g$-composable sequences with rate $r \le \frac{0.433}{(n+1)^{1.27\dots}},$ where
                            $1.27\dots = \log_2(1+\sqrt{2})$.
                            </p>
                        </div>
                        <ul class=fragment>
                            <li> First work showing acceleration for gradient descent with respect to gradient norm metric.</li>
                            <li> We find sequences in the short step size regime which recover those recently found in Rotaru et al.</li>
                        </ul>
                    </section>
                    <section data-auto-animate>
                        <h3> Basic Sequences</h3>
                        <p> A <b>basic</b> $s$-composable sequence is either the empty sequence or a sequence that can be expressed as the composition of two smaller basic $s$-composable sequences. </p>
                        <p> A <b>basic</b> $f$-composable sequence is one that can be constructed by repeatedly composing the empty sequence with basic $s$ composable sequences. </p>
                        <p> We can enumerate such basic sequences and ask what the best possible rate is for a length $n$ basic sequence. This can be computed by dynamic program </p>
                    </section>
                    <section data-auto-animate>
                        <h3> Basic Sequences</h3>
                        <p><b> Examples </b><p>
                        <table>
                            <tr>
                                <td> <img src="3_02.png" width=300px/></td>
                                <td> <img src="4_08.png" width=300px/></td>
                            </tr>
                            <tr>
                                <td> <img src="5_25.png"  width=300px/></td>
                                <td> <img src="best_20.png" width=300px/></td>
                            </tr>
                        </table>
                    </section>
                    <section data-auto-animate>
                        <h3> Basic Sequences</h3>
                        <p><b> Rate </b><p>
                        <img src="rates.png"/>
                    </section>
                    <section data-auto-animate>
                        <h2> Certificates </h2>
                    </section>
                    <section data-auto-animate>
                        <h3> Recursive Certificates </h3>
                        <div style="flex:1;margin:0.25in;text-align:left; padding:0.35in; font-size:0.75em; background:#EEEEFF">
                        <h4> Theorem </h4>
                            <p style="font-size:0.75em">
                                If $h$ is $s$-composable with rate $r_1$ and $\ell$ is $f$-composable with rate $r_2$, then there is a constant $\mu$ so that
                                \[
                                    [h, \mu, \ell]
                                \]
                                is $f$-composable with rate $\frac{2r_1r_2}{r_1 + 4r_2 + \sqrt{r_1^2+8r_1r_2}}$.
                            </p>
                        </div>
                        <p>
                            To prove the rate for $[h, \mu, \ell]$, we use PEP certificates for the appropriate inequalities for both $h$ and $\ell$ and combine them together.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h3> Recursive Certificates </h3>
                        <p>
                            Assume $\ell$ is $f$-composable with rate $r$,
                            \[
                                f(x_n) - f(x_*) \le \frac{r}{2}\|x_0 - x_*\|^2,
                            \]
                            we can extract a PEP proof of this fact.
                        </p>
                        <div class=fragment>
                            <p>
                            By examining that proof, we can show that there exists some sequence $\nu \in \R^{n+1}$ so that we get the (equivalent) inequality
                        </p>
                            <p style="color:blue">
                            If $x_0, \dots, x_n$ are obtained by applying the step size sequence $\ell$, then
                            \[
                                \sum_{i=0}^n \nu_i (2(f_i-f_n)+\|\nabla f(x_i)\|^2+2\langle \nabla f(x_i), x_0 - x_i\rangle)-\|\sum_{i=0}^n \nu_i\nabla f(x_i)\|^2 \ge 0.
                            \]
                            </p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h3> Recursive Certificates </h3>
                        <p style="color:blue">
                            If $x_0, \dots, x_n$ are obtained by applying the step size sequence $\ell$, then
                            \[
                                \sum_{i=0}^n \nu_i (2(f_i-f_n)+\|\nabla f(x_i)\|^2+2\langle \nabla f(x_i), x_0 - x_i\rangle)-\|\sum_{i=0}^n \nu_i\nabla f(x_i)\|^2 \ge 0.
                            \]
                        </p>
                        <p><b> Advantages </b></p>
                        <ul>
                            <li>
                                This new inequality still holds even if we shift the index of $x_i$, so we can apply this inequality to the sequence $[h, \mu, \ell]$ (where $\ell$ is only applied starting after some number of steps have already been taken).
                            </li>
                            <li>
                                It is also close to `separable', so that it is a sum of terms, each of which only depends on a single iterate $x_i$.
                            </li>
                        </ul>
                    </section>
                    <section data-auto-animate>
                        <h3> Recursive Certificates </h3>
                        <p style="color:blue">
                            \[
                                \sum_{i=0}^n \nu_i (2(f_i-f_n)+\|\nabla f(x_i)\|^2+2\langle \nabla f(x_i), x_0 - x_i\rangle)-\|\sum_{i=0}^n \nu_i\nabla f(x_i)\|^2 \ge 0.
                            \]
                        </p>
                        <p>Take this inequality and shift the indices:</p>
                        <p style="color:red;margin-left:-10px">
                            \[
                            \sum_{i=n+1}^{n'} \nu_i' (2(f_i-f_{n'})+\|\nabla f(x_i)\|^2+2\langle \nabla f(x_i), x_{n+1} - x_i\rangle)-\|\sum_{i=n+1}^{n'} \nu_i'\nabla f(x_i)\|^2 \ge 0.
                            \]
                        </p>
                        <div  class = fragment>
                        <p>Compare this to the following (desired) inequality:</p>
                        <p style="color:green;margin-left:-10px">
                            \[
                            \sum_{i=0}^{n'} \nu_i' (2(f_i-f_{n'})+\|\nabla f(x_i)\|^2+2\langle \nabla f(x_i), x_{0} - x_i\rangle)-\|\sum_{i=0}^{n'} \nu_i'\nabla f(x_i)\|^2 \ge 0.
                            \]
                        </p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h3> Recursive Certificates </h3>
                        <p> The $s$-composability definition is exactly the one that is needed so that when we linearly combine the inequality guaranteed by $s$-composability to the one guaranteed by $f$-composability, we get the desired extended $f$-composability inequality (up to a small correction from the middle step). </p>
                        <p class=fragment> The fact that Huber and quadratic are tight for $f$-composable sequences furnishes additional algebraic identities needed to combine cleanly. </p>
                        <p class=fragment> $g$ composable sequences have a similar recursive structure in their certificates . </p>
                    </section>
                    <section>
                        <h1> Thanks for listening!</h1>
                        <p> Grimmer, Benjamin, Kevin Shu, and Alex Wang. "Accelerated Objective Gap and Gradient Norm Convergence for Gradient Descent via Long Steps." arXiv preprint arXiv:2403.14045 (2024).</p>
                    </section>
            </section> </div>

		</div>

		<script src="reveal.js"></script>
		<script src="math.js"></script>
		<script>
			Reveal.initialize({
				history: true,
				transition: 'linear',

				mathjax2: {
					config: 'TeX-AMS_HTML-full',
					TeX: {
						Macros: {
							R: '\\mathbb{R}',
							E: '\\mathbb{E}',
							set: [ '\\left\\{#1 \\; ; \\; #2\\right\\}', 2 ]
						}
					}
				},

				// There are three typesetters available
				// RevealMath.MathJax2 (default)
				// RevealMath.MathJax3
				// RevealMath.KaTeX
				//
				// More info at https://revealjs.com/math/
				plugins: [ RevealMath.MathJax2 ]
			});
            Reveal.addEventListener( 'drawCurve', function() {
                element = document.getElementById("curveDrawing");
                element.classList.add("path");
            } );
		</script>

	</body>
</html>
