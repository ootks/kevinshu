Title: The Surprising Ubiquity of Convex Optimization
Abstract:
Computational optimization plays an increasingly central role in society, making it necessary to develop efficient algorithms that can offer rigorous guarantees on their outputs. Convex optimization is the study of a particular subset of optimization problems which can often be tractably solved to global optimality. Not all optimization problems are convex, but it is often possible to change the presentation of the problem to yield an equivalent convex reformulation. This talk will discuss a general framework for finding such reformulations based on a topological concept we call 'Lagrangian dual sections'. One application of our method is to the analysis of nonconvex optimization problems over manifolds that can be exactly reformulated as convex optimization problems. We will also discuss applications of convex reformulations to the design and analysis of first-order optimization algorithms. These reformulations have led to the discovery of sequences of step sizes for gradient descent resulting in faster asymptotic convergence than constant step sizes for smooth convex functions, as well as the development of methods with strong performance guarantees based on game-theoretic concepts.
