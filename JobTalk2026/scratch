                    <section>
                        <h4> Certificates from Convex Optimization </h4>
                        <p> Convex optimization can be used to <i>certify</i> that there is no point satisfying certain conditions, as long as those conditions.</p>
                        <p class="fragment"> If no, then the algorithm always succeeds. </p>
                    </section>
                    <section data-auto-animate>
                        <h3> Why Convex Optimization? </h3>
                        <ul>
                            <li class ="fragment"><p style="font-weight:bold; font-size:40px"> Fast, Well-Established Algorithms </p></li>
                            <li class ="fragment"><p style="font-weight:bold; font-size:40px"> Robust Theory </p></li>
                            <li class ="fragment"><p style="font-weight:bold; font-size:40px"> Global Optimality Guarantees </p></li>
                        </ul>
                        <p class="fragment" style="color:red; font-size:40px"> But not all optimization problems are convex! </p>
                    </section>
                </section>
                <section>
                    <section data-auto-animate>
                        <h3> What is Convex Optimization? </h3>
                        <p class=alignLeft> Convex geometry is the natural generalization of linear algebra that admits inequalities. </p>
                        <div  style="display:flex">
                            <div>
                                <p class="fragment alignLeft"> <b>Convex Set </b> - A set defined by a system of linear inequalities. </p>
                                <p class="fragment alignLeft"> <b>Convex Function </b> - A function which is the pointwise minimum of linear functions. </p>
                                <p class="fragment alignLeft"> <b>Convex optimization</b> - The study of minimizing convex functions on convex sets.  </p>
                            </div>
                            <img height=200px width=200px src="convex_function.png">
                        </div>


                    </section>
                    <section data-auto-animate>
                        <h3> Why Convex Optimization? </h3>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> Projection Simplicity</h4>
                        <p class=alignLeft><b> Optimization problems, even over complicated domains, often only depend on a few salient features of what you are optimizing. </b></p>
                        <p class=alignLeft> Focusing on these features make it is possible to find simpler (convex) formulations of the problem. </p>
                        <p class="alignLeft fragment"> How general is this phenomenon; can it be discovered and exploited automatically?</p>
                    </section>


                    <section data-auto-animate>
                        <h3> Hyperbolic polynomials for linear algebra<sup>1, 2</sup>  </h3>

                        <p style="font-size:30px; margin-top:200px; text-align:left">1. <i>Hyperbolic Relaxation of k-Locally Positive Semidefinite Matrices</i> - G Blekherman, S Dey, S Sun</p>
                        <p style="font-size:30px; text-align:left">2. <i> Debiasing Polynomial and Fourier Regression</i> - C Cama√±o and R Meyer</p>
                    </section>
                    <section data-auto-animate>
                        <h4> Hyperbolic polynomials </h4>
                        <ul>
                            <li> Polynomials with real rootedness properties </li>
                            <li> Arise in convex optimization, sampling theory, combinatorics </li>
                        </ul>
                    </section>
                    <section data-auto-animate>
                        <h4> Debiasing Polynomial and Fourier Regression </h4>
                        <ul>
                            <li> <b>Active sampling</b> - how do you find a polynomial approximation of a function $f$ <i>without knowing $f$ explicitly</i>?</li>
                            <li class=fragment> Given an oracle for computing $f(x)$, how do you find the best polynomial approximation of $f$ in the $L_{\mu}$ norm using as few evaluations as possible? </li>
                            <li class=fragment> A surprising algorithm: sample a random matrix $X$ from the $\mu$-unitary ensemble, compute its eigenvalues $\lambda_0, \dots, \lambda_d$, and interpolate a polynomial $\hat{p}$ so that $\hat{p}(\lambda_i) = f(\lambda_i)$ for each $i$. $\E[\hat{p}] = p^*$. </li>
                        </ul>
                    </section>
                    <section data-auto-animate>
                        <h4> Approximate PSD checking </h4>
                        <ul>
                            <li> <b>Approximate PSD checking</b> - If you know that all $k\times k$ submatrices of an $n\times n$ matrix are PSD, how far is that matrix from being PSD?</li>
                            <li class=fragment> We show that the worst case always has all equal diagonal entries and all equal off-diagonal entries for an arbitrary matrix norm distance metric using a convex relaxation from hyperbolic polynomials.</li>
                        </ul>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> An Example for Stiefel Manifold</h4>
                        <p> When is there a subspace of $\R^{n \times m}$ of dimension $k$ containing no singular matrix? </p>
                        <p> When $k \lt n-m,$ a generic such subspace has this property. </p>
                        <div style="background-color:lightgray; padding:30px;">
                            <h4 style="text-align:left;padding-left:50px"> Theorem (Adams, Lax, Phillips 1965)</h4>
                            <p class=alignLeft> A $k$ dimensional subspace of $\R^{n\times n}$ containing no nonzero singular matrix exists if and only if $k \le \rho(n)$. If $n = (2a+1)2^{c+4d}$ with $c \lt 4$, then $\rho(n)$ is the Radon-Hurwitz number
                            \[
                            \rho(n) = 2^c + 8d.
                            \]
                            </p>
                        </div>
                        <p>$\rho(n)$ is connected to representation theory of Clifford algebras, vector bundles on spheres. </p>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> Connections to Lie group theory and Topology  </h4>
                        <p> In examples where $M$ is the <b>orbit of a Lie group action </b>, and the constraints are <b>linear</b>, this is closely related to Kostant's convexity theorem.
                        </p>
                        <p class=fragment> Kostant's convexity theorem characterizes the maximizers of the Lagrangian.  </p>
                        <p class=fragment> Hidden convexity is implied if the constraints span a <b> noncrossing subspace</b> where certain generalized singular values are sufficiently generic .</p>
                    </section>

                            After making queries $x_0, \dots, x_n$, for <b>any</b> oracle responses $(f(x_i), \nabla f(x_i))_{i=0}^n$, the algorithm produces a number $\tau$ (depending on the responses) and guarantees that 
                            \[
                                \SUBOPT \le \tau.
                            \]
                            Moreover, for any sequence of queries, there exists a $L$-smooth convex function $f^*$ so that any algorithm making the same initial queries,
                            \[
                                \SUBOPT \ge \tau.
                            \]


                    <section data-auto-animate>
                        <h4 class=alignLeft> Example of Hidden Convexity</h4>
                        <div style="background-color:lightgray;padding-left:30px;padding-right:30px; padding-top:30px;">
                            <h4 style="text-align:left;padding-left:50px"> Theorem (Brickman) </h4>
                            <p>
                            $\{(x^{\intercal}A_0x, x^{\intercal}A_1x) : \|x\|=1\} \subseteq \R^2$ is convex.
                            </p>
                        </div>
                        <video controls><source src="QuadraticSphere.mp4" type="video/mp4"></video>(Brickman)  
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> Example of Hidden Convexity</h4>
                        <div style="background-color:lightgray; padding:30px;">
                            <h4 style="text-align:left;padding-left:50px"> Theorem (Brickman)  </h4>
                            <p>
                            $\{(x^{\intercal}A_0x, x^{\intercal}A_1x) : \|x\|=1\} \subseteq \R^2$ is convex.
                            </p>
                        </div>
                        <br>
                        <div style="background-color:lightgray; padding:30px;" class=fragment>
                            <h4 style="text-align:left;padding-left:50px"> Corollary (Homogenized S-lemma)</h4>
                            <p>
                                Any QCQP of the form
                                \[
                                    \max \{x^{\intercal}A_0x : x^{\intercal}A_1x = c_1, x^{\intercal}A_2x = c_2\}
                                \]
                                has a tight convex relaxation.
                            </p>
                        </div>
                    </section>
                    <section>
                        <h4> Performance Estimation Problems </h4>
                        <p> Formally, define error metric </p>
                        <p>

                        \[
                            \SUBOPT = \frac{f(x_N) - f_{min}}{\|x_0 - x_{\star}\|^2}?
                        \]
                        </p>
                        <p> And optimization problem </p>
                        <table>
                            <th></th>
                            <tr style="border-bottom:none;"> <td> max </td> <td> $\SUBOPT$ </td> </tr>
                            <tr> <td> s.t. </td> <td> $f$ is $L$-smooth and convex. </td> </tr>
                        </table>
                        <p class=fragment>Solve this by <b>reformulating</b> as a convex program via <b>interpolation conditions</b> (Taylor, Hendrickx and Glineur, 2017)</p>.
                    </section>
                    <section>
                        <h4> First-Order Methods</h4>
                        <p> <b>Meta-goal:</b> Find a first-order method with good <b>worst-case behavior</b>.</p>
                        <p>Different choices of $h$'s lead to different convergence rates.</p>
                        <p> <b>Subquestion:</b> How can we tell if a given first-order method converges quickly?</p>
                    </section>
                    <section>
                        <h4> Worst-Case Instance Search</h4>
                        <p> We will restrict attention to convex, $L$-smooth functions. </p>
                        <p> <b> $L$-smooth </b> - gradient is $L$-Lipschitz under Euclidean distance. </b>
                        <p> 
                        Suppose that the $h_{ij}$ are <b>chosen in advance</b>; we want to find a `bad function' $f$ so that $f(x_n)$ is much larger than $f_{min}$ (relative to the initial error).
                        </p>
                        <p class="fragment">
                        Given an algorithm for choosing the $x_i$, can we find $f$ maximizing
                        \[
                            \SUBOPT = \frac{f(x_N) - f_{min}}{\|x_0 - x_{\star}\|^2}?
                        \]
                        </p>
                        <p class="fragment">
                        Yes! Using convex optimization.  (Drori and Teboulle, 2012) The resulting objective is the worst case rate of convergence for the algorithm. (Taylor, Hendrickx and Glineur, 2017)
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Performance Estimation Papers </h4>
                    </section>
                    <section data-auto-animate>
                        <h4> Performance Estimation Papers </h4>
                        <ul>
                            <li> <b> D. Kim and J. Fessler, "Optimized first-order methods for smooth convex minimization." Mathematical programming (2016) </b> </li>
                            <li> A. Taylor, and Y. Drori. "An optimal gradient method for smooth strongly convex minimization." Mathematical Programming (2023)</li>
                            <li> S. Das Gupta, B. Van Parys, and E. Ryu. "Branch-and-bound performance estimation programming: A unified methodology for constructing optimal optimization methods" Mathemathical Programming, (2023).</li>
                        </ul>
                    </section>
