<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title> The Surprising Ubiquity of Convex Optimization </title>

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="reveal.css">
		<link rel="stylesheet" href="style.css">
		<link rel="stylesheet" href="solarized.css" id="theme">
        <style>
            li {font-size: 30px}
            .alignLeft {text-align:left;}
        </style>
	</head>

	<body>

		<div class="reveal">

			<div class="slides">

                <section>
                    <h3> The Surprising Ubiquity of</h3>
                        <h3>Convex Optimization </h3>
                    <div>
                        <!--<img src="sphere_proj.png" height="300" width="400" />-->
                    </div>
                    <div style="display:flex">
                        <p style="font-size:30px;flex:50%; padding-top:25px"> Kevin Shu </p> 
                        <div  style="flex:50%">
                        <img src="caltech.png" width=200px />
                        </div>
                    </div>
                </section>
                <section>
                    <section>
                        <h4> Optimization is Everywhere </h4>
                        <div style="display:flex">
                            <div style="flex:1; width:30%">
                                <h5> Robotics </h5>
                                <img src ="slam.png" style="box-shadow: 0px 0px 30px #ddd; margin-top:50px;margin-bottom:120px;">
                                <p class=centered style="font-size:20px; padding-left:10px">
                                    Rosen et al. 2021.
                                </p>
                            </div>
                            <div style="flex:1; width:30%">
                                <h5> Materials </h5>
                                <img src ="metamaterial.png" style="box-shadow: 0px 0px 30px #ddd">
                                <p class=centered style="font-size:20px; margin-top:50px; padding-left:10px">
                                 Serles et al. 2025.
                                </p>
                            </div>
                            <div style="flex:1; width:30%">
                                <h5> Language Models </h5>
                                <img src ="chatgpt.png" style="box-shadow: 0px 0px 30px #ddd; margin-top:50px;">
                            </div>
                        </div>
                    </section>
                    <section>
                        <h4> Issues with Optimization </h4>
                        <div style="display:flex">
                            <img src="self_driving.png" style="flex:1; width:30%; padding-right:20px;">
                            <img src="ai_hallucination.png" style="flex:1; width:30%">
                        </div>
                    </section>
                    <section>
                        <h4> Desiderata for Optimizers </h4>
                        <ul>
                            <li style="margin-top:20px;"> Computationally efficient </li>
                            <li style="margin-top:20px;"> Reliable </li>
                            <li style="margin-top:20px;"> Expressive </li>
                            <li class=fragment style="margin-top:20px;"> Globally optimal </li>
                        </ul>
                    </section>
                    <section>
                        <h3> What is Convex Optimization? </h3>
                        <div  style="display:flex">
                            <div>
                                <p class="alignLeft"> <b>Convex Set</b> - For any $x, y \in C$, and $t \in [0,1]$,
                                \[
                                    tx + (1-t)y \in C.
                                \]
                                </p>
                                <p class="alignLeft"> <b>Convex Function</b> - For any $x, y \in \R^d$, and $t \in [0,1]$,
                                \[
                                    f(tx + (1-t)y) \le tf(x) + (1-t)f(y).
                                \]
 </p>
                            </div>
                            <img height=200px width=200px src="convex_function.png">
                        </div>
                    </section>
                    <section>
                        <h3> What is Convex Optimization? </h3>
                        <div  style="display:flex">
                            <div style="margin:20px;">
                                <p style="text-align:center"> Linear Programs </p>
                                <img src="linear_programming.svg" height=150px/>
                                <table style="font-size:30px">
                                    <tr><td> $\min$ </td><td> $c^{\intercal} x$</td></tr>
                                    <tr><td> s.t. </td><td> $Ax = b$</td></tr>
                                    <tr><td> </td><td> $x \in \R^n_{\ge 0}$</td></tr>
                                </table>
                            </div>
                            <div style="margin:20px;" class=fragment>
                                <p style="text-align:center"> Conical Programs </p>
                                <img src="conical.png" height=150px/>
                                <table style="font-size:30px">
                                    <tr><td> $\min$ </td><td> $c^{\intercal} x$</td></tr>
                                    <tr><td> s.t. </td><td> $Ax = b$</td></tr>
                                    <tr><td> </td><td> $x \in K$</td></tr>
                                </table>
                                <p> e.g. $K$ is PSD cone.</p>
                            </div>
                            <div style="margin:20px;" class=fragment>
                                <p style="text-align:center"> Regression </p>
                                <img src="regressoin.png" height=150px/>
                                <table style="font-size:30px">
                                    <tr><td> $\min$ </td><td> $\|Ax - b\|_p$</td></tr>
                                    <tr><td> s.t. </td><td> $x \in \R^n$</td></tr>
                                </table>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h4> Why Convex Optimization? </h4>
                        <ul style="color:var(--pastel-green)">
                            <li> Computationally efficient </li>
                            <li> Reliable </li>
                            <li> Expressive </li>
                            <li> Globally optimal </li>
                        </ul>
                        <p class="fragment" style="color:var(--pastel-red); font-size:30px; text-align:center"> But not all optimization problems are convex! </p>
                    </section>
                    <section data-auto-animate>
                        <p> We want to solve some problem in engineering or science. </p>
                            <img height=300px src="landscape.png"/>
                    </section>
                    <section data-auto-animate>
                        <h4>Convex Reformulations</h4>
                        <p> We want to solve some problem in engineering or science. </p>
                        <div style="display:flex;">
                            <img height=300px src="landscape.png"/>
                            <p style="font-size:50px;margin-top:100px;">&#8594;</p>
                            <img height=300px src="landscape_convexified.png"/>
                        </div>
                    </section>
                </section>
                <section>
                    <section data-auto-animate>
                        <h4> Manifestations of hidden convexity in my work </h4>
                        <img src="spoke_and_hub.png"/>
                    </section>
                    <section data-auto-animate>
                        <h4> Manifestations of hidden convexity in my work </h4>
                        <img src="spoke_and_hub2.png"/>
                    </section>
                    <section data-auto-animate>
                        <h4> Outline of the Talk </h4>
                        <ol>
                            <li> <b>Introduction</b> </li>
                            <br>
                            <li> A Topological Framework for Hidden Convexity </li>
                            <ul>
                                <li> Lagrangian Dual Sections </li>
                                <li> Path Tracking Algorithms </li>
                            </ul>
                            <br>
                            <li> Automatic Design of First-Order Methods</li>
                            <ul>
                                <li> Long Step Gradient Descent</li>
                                <li> Subgame Perfect Gradient Method</li>
                            </ul>
                            <br>
                            <li> Conclusions and Future Work</li>
                        </ol>
                    </section>
                </section>
                <section>
                    <section data-auto-animate>
                        <h3> Convexifying Optimization Problems with Topology<sup>1</sup>  </h3>

                        <p style="font-size:30px; margin-top:200px; text-align:left">1. <i>Lagrangian Dual Sections: A Topological View of Hidden Convexity</i> - V Chandrasekaran, T Duff, J Rodriguez, <span style="color:#6A8DB5">K Shu</span> </p>
                    </section>
                    <section>
                        <h4> Motivating Example: Attitude Estimation </h4>
                        <p> Probes in deep space need to estimate their orientation.<sup>1</sup> </p>
                        <p class=caption>1. Wahba, Grace. "A least squares estimate of satellite attitude." SIAM review 7.3 (1965): 409-409.</p>
                        <img height=300px src="sattelite_image.webp"/>
                        <p class=fragment> Two data sources: <b>positions</b> of distant stars and accelerometer <b> estimate </b>. </p>
                    </section>
                    <section>
                        <h4> Attitude Estimation </h4>
                        <p> <b> Goal: </b> Find a rotation satisfying two conditions: </p>
                        <ol>
                            <li> Transforms an internal star map to observed locations of the stars. </li>
                            <li> Is not too far from some fixed rotation matrix. </li>
                        </ol>
                        <p class=fragment> The set of rotations is called the <b> special orthogonal group </b> $\SO(3)$. 
                        \[
                        \SO(n) = \{X \in \R^{n \times n} : X^{\intercal}X = I, \det(X) = 1\}.
                        \]
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4> Attitude Estimation </h4>
                        <p> <b> Model: </b> </p>
                        <table>
                            <tr> <td> $\min_X$ </td> <td> $\|AX - B\|^2$ </td> </tr>
                            <tr> <td> s.t. </td> <td> $\|X - X_0\|^2 \le \epsilon$ </td> </tr>
                            <tr> <td> </td> <td> $X \in \SO(n)$ </td> </tr>
                        </table>
                        <p>$A$ and $B$ represent the observed/stored star locations.</p>
                        <p>$X_0$ is estimated rotation matrix.</p>
                        <p class=fragment> Algebra shows that objective is equivalent to $\tr(B^{\intercal}A X)$ and first constraint is equivalent to $\tr(I - X_0^{\intercal}X) \le \epsilon$.</p>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> Hidden Convexity and $\SO(n)$</h4>
                        <div style="background-color:lightgray; padding:30px;">
                            <h4 style="text-align:left"> Theorem (R<span style="color:var(--pastel-blue)">S</span>W 2024)</h4>
                            <p class=alignLeft> If $A_0, A_1 \in \R^{n\times n}$ for $n > 2$, then 
                            \[ \big\{(\tr( A_0^{\intercal} X), \tr( A_1^{\intercal} X)) : X \in \SO(n)\big\} \subseteq \R^2\] 
                            is convex. </p>
                        </div>
                        <p> Means we can apply convex optimization to the constrained problem. </p>
                    </section>
                    <section data-auto-animate>
                        <p style="text-align:left"> Many fundamental optimization problems can be expressed as <i>constrained</i> problems over nonconvex sets (e.g. manifolds, algebraic varieties). </p>
                        <div class="fragment" style="display:flex">
                            <div style="background-color:lightgray; padding:30px;">
                                <h4 style="text-align:left"> Examples: </h4>
                                <ul >
                                    <li><p> Quadratically Constrained Quadratic Programming (QCQP)</p>
                                        <p style="font-size:20px">Combinatorics, Power Systems</p>
                                    </li>
                                    <li><p> Stiefel Manifold Optimization </p>
                                        <p style="font-size:20px">Robotics, Computer Vision</p>
                                    </li>
                                    <li><p> Inverse Eigenvalue Problems  </p>
                                        <p style="font-size:20px">Spectral Graph Theory, Network Science, Sturm-Liouville Equations </p>
                                    </li>
                                </ul>
                            </div>
                            <div>
                            <img width=300px src="rotating surface.gif"/>
                            <p style="padding-left:20px; font-size:20px;"> Boyd's embedding of $\R\mathbb{P}^2$. </p>
                            </div>

                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4> Constrained Optimization</h4>
                        <p style="text-align:left"> Many fundamental problems expressed as <i>constrained</i> optimization problems over a domain $M$ (e.g. a manifold, an algebraic variety, $\R^d$). </p>
                        <div style="display:flex">
                            <div style="background-color:lightgray; padding:30px;">
                                <h4 style="text-align:left"> Formulation </h4>
                                <p>
                                \[
                                    \max \{f_0(x) : f_1(x) = c_1, \dots, f_k(x) =c_k, x \in M\},
                                \]
                                </p>
                                <p>for topological space $M$ and $f : M \rightarrow \R^{k+1}$.</p>
                            </div>
                            <img width=300px src="rotating surface.gif" data-id="surface"/>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4> Constrained Optimization</h4>
                        <p class=alignLeft> Such problems are typically NP-hard. Convex optimization gives bounds on the optimal value.</p>
                        <div style="background-color:lightgray; padding:30px; margin-bottom:20px">
                            <h4 style="text-align:left"> Formulation </h4>
                            <p>
                            \[
                                \max \{f_0(x) : f_1(x) = c_1, \dots, f_k(x) =c_k, x \in M\},
                            \]
                            </p>
                            <p>for topological space $M$ and $f : M \rightarrow \R^{k+1}$.</p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4> Constrained Optimization</h4>
                        <div style="background-color:lightgray; padding:30px; margin-bottom:20px">
                            <h4 style="text-align:left"> Formulation </h4>
                            <p>
                            \[
                                \max \{f_0(x) : f_1(x) = c_1, \dots, f_k(x) =c_k, x \in M\},
                            \]
                            </p>
                            <p>for some set $M$ and a function $f : M \rightarrow \R^{k+1}$.</p>
                        </div>
                        <div class="fragment" style="background-color:lightgray; padding:30px;">
                            <h4 style="text-align:left"> Reformulation in terms of the image</h4>
                            <p>
                            \[
                                \max \{y_0 : y_1 = c_1, \dots, y_k =c_k, y \in f(M)\},
                            \]
                            </p>
                            <p>for some set $M$ and a function $f : M \rightarrow \R^{k+1}$.</p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4> Constrained Optimization</h4>
                        <div style="background-color:lightgray; padding:30px; margin-bottom:20px">
                            <h4 style="text-align:left"> Formulation </h4>
                            <p>
                            \[
                                \max \{f_0(x) : f_1(x) = c_1, \dots, f_k(x) =c_k, x \in M\},
                            \]
                            </p>
                            <p>for some set $M$ and a function $f : M \rightarrow \R^{k+1}$.</p>
                        </div>
                        <div style="background-color:lightgray; padding:30px;">
                            <h4 style="text-align:left"> Convex Relaxation </h4>
                            <p>
                            \[
                            \max \{y_0 : y_1 = c_1, \dots, y_k =c_k, y \in \color{red}{\text{conv}(}f(M)\color{red}{)}\},
                            \]
                            </p>
                            <p data-id="relax_text">for some set $M$ and a function $f : M \rightarrow \R^{k+1}$.</p>
                        </div>
                        <p class=fragment> When are these two optimization problems equivalent? </p>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> Tightness of Convex Relaxations</h4>
                        <p class=alignLeft> <i> If $M$ is a topological space (e.g. a manifold, an algebraic variety, $\R^d$), and $f : M \rightarrow \R^{k+1}$ is continuous, when is the convex relaxation tight?</i></p>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> Tightness of Convex Relaxations</h4>
                        <p class=alignLeft> <i> If $M$ is a topological space (e.g. a manifold, an algebraic variety, $\R^d$), and $f : M \rightarrow \R^{k+1}$ is continuous, when is the convex relaxation tight?</i></p>
                        <p class=alignLeft>We can give this an answer in terms of the Lagrangian 
                        $$ \mathcal{L}(\lambda, x) = \langle \lambda, f(x)\rangle.$$</p>
                        <p class=alignLeft>Associate for each $\lambda \in \R^{k+1}$ the optimization problem of finding
                        $$ \underset{x\in M}{\argmax}\; \mathcal{L}(\lambda, x)$$</p>
                        <p class="alignLeft fragment"><i>How do the maximizers depend on the choice of Lagrange multiplier $\lambda$?</i></p>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> Tightness of Convex Relaxations </h4>
                        <p class=alignLeft> <i> If $M$ is a topological space (e.g. a manifold, an algebraic variety, $\R^d$), and $f : M \rightarrow \R^{k+1}$ is continuous, when is the convex relaxation tight?</i></p>
                        <p class="alignLeft"><i>How do the maximizers of $\mathcal{L}(\lambda, x) = \langle \lambda, f(x)\rangle$ depend on the choice of Lagrange multiplier $\lambda$?</i></p>
                        <div class=fragment>
                        <div style="background-color:lightgray; padding:30px;">
                            <h4 style="text-align:left">Definition </h4>
                        <p> A <span style="color:var(--pastel-blue)">Lagrangian dual section</span> for $f$ consists of a <b>continuous</b> function $D : \R_{\ge 0} \times \R^k \rightarrow M$ so that for all $\lambda$,
                        \[
                        D(\lambda) \in \underset{x \in M}{\argmax}\; \L(\lambda,x).
                        \]
                        </p>
                        </div>
                        <p> That is, $D(\lambda)$ maximizes $\L(\lambda,x)$ for all $\lambda \in \R_{\ge 0} \times \R^k$. </p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> Tightness of Convex Relaxations</h4>
                        <p class=alignLeft> <i> If $M$ is a topological space (e.g. a manifold, an algebraic variety, $\R^d$), and $f : M \rightarrow \R^{k+1}$ is continuous, when is the convex relaxation tight?</i></p>
                        <div style="background-color:lightgray; padding:30px;" class=fragment>
                            <h4 style="text-align:left">Theorem (CDR<span style="color:var(--pastel-blue)">S</span> 2025) </h4>
                            <p class=alignLeft>
                            Suppose that there is a <span style="color:var(--pastel-blue)">Lagrangian dual section</span> for $f$. Then for any $c \in \R^k$, 
                                \[
                                    \max \{f_0(x) : f_1(x) = c_1, \dots, f_k(x) = c_k, x\in M\} = 
                                \]
                                \[
                                \max \{y_0 : y_1 = c_1, \dots, y_k = c_k, y \in \color{red}{\conv}(f(M))\}.
                                \]
                            </p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> Tightness of Convex Relaxations</h4>
                        <p class=alignLeft> Recovers a wide range of results with a unified proof </p>
                        <ul>
                            <li> O. Toeplitz, "Das algebraische Analogon zu einem Satze von Fejér," Mathematische Zeitschrift (1918)</li>
                            <li> F. Hausdorff, "Der wertvorrat einer bilinearform," Mathematische Zeitschrift (1919)</li>
                            <li> T.-Y. Tam, “Kostant’s convexity theorem and the compact classical groups,” Linear and Multilinear Algebra (1997)</li>
                            <li> C.-K. Li and T.- Y. Tam, “Numerical ranges arising from simple lie algebras,” Canadian Journal of Mathematics (2000)</li>
                            <li> E. Gutkin, E. A. Jonckheere, and M. Karow, “Convexity of the joint numerical range: Topological and differential geometric viewpoints,” Linear Algebra and its Application (2004)</li>
                            <li> Mengmeng Song and Yong Xia. Linear programming on the Stiefel manifold.  SIAM Journal on Optimization (2024)
                        </ul>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> An Example for Stiefel Manifold</h4>
                        <p class=alignLeft> Stiefel manifold - orthogonal projections</p>
                        <p class=alignLeft> \[\St^{n,m} = \{X \in \R^{n\times m} : X^{\intercal}X = I\}\]</p>
                        <div style="background-color:lightgray; padding:30px;">
                            <h4 style="text-align:left"> Theorem (CDR<span style="color:var(--pastel-blue)">S</span> 2025)</h4>
                        <p class=alignLeft> If $A_0, \dots, A_k$ span a linear subspace of $\R^{n\times m}$ containing no nonzero singular matrix (i.e. matrix of rank $< m$), then 
                           \[ \max \{\langle A_0, X\rangle : \langle A_1, X\rangle = c_1, \dots, \langle A_k, X\rangle = c_k, X \in \St^{n,m} \}=\] </p>
                        <p>\[ \max \{\langle A_0, X\rangle : \langle A_1, X\rangle = c_1, \dots, \langle A_k, X\rangle = c_k, \color{red}{\sigma_{max}(X) \le 1}\}.\] </p>
                        </div>
                        <p> Similar results in the context of QCQPs, inverse eigenvalue problems, and more. </p>
                    </section>
                    <section>
                        <h4> Some Mathematical Connections </h4>
                        <ul>
                            <li> <p> Representations of Clifford algebras </p></li>
                            <li> <p> Vector bundles on spheres </p></li>
                            <li><p> Orbits of Lie group actions</p></li>
                            <li> <p>Kostant convexity theorem</p></li>
                            <li> <p>Von Neumann-Wigner noncrossing theorem</p></li>
                        </ul>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> From Continuity to Path-tracking Algorithms </h4>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> From Continuity to Path-tracking Algorithms </h4>
                        <div style="display:flex">
                            <div>
                                <p> Does the existence of a Lagrangian dual section $D(\lambda)$ imply fast algorithms? </p>
                                <p class=fragment> Knowing $D(\lambda)$ for all $\lambda$ facilitates the ellipsoid algorithm. </p>
                                <p class=fragment> Goal: Find $D(\lambda)$ for arbitrary $\lambda$. </p>
                            </div>
                            <img src="path_trackinh.gif"/>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> From Continuity to Path-tracking Algorithms </h4>
                        <p> Assume there is a Lagrangian dual section $D$, and that we know $D(\lambda_0)$ for fixed $\lambda_0 \in \R^{k+1}$.</p>
                        <p> Try a <i>path-tracking</i> approach to find $D(\lambda)$ for $\lambda \neq \lambda_0$. </p>
                        <div class=fragment>
                            <p> <b>Idea: </b> Compute $D((1-t)\lambda + t\lambda_0)$ for $t \in [0,1]$ by slowly changing $t$.</p>
                            <img src="lds_path.png" height=600px/>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> From Continuity to Path-tracking Algorithms </h4>
                        <p>
                        Instantiating this idea on a computer requires notions from <b>Riemannian geometry</b>. We will assume
                        </p>
                        <ol>
                            <li> $M$ is a Riemannian manifold. </li>
                            <li> $f$ is smooth. </li>
                        </ol>
                        <div class=fragment>
                        <div style="background-color:lightgray;padding:30px;">
                            <h4 style="text-align:left"> Theorem (CDR<span style="color:var(--pastel-blue)">S</span> 2025) - Informal </h4>
                            <p> 
                            Assume (1) and (2), and that there is a Lagrangian dual section $D$. Given the value of $D(\lambda_0)$ for some $\lambda_0 \in \R^{k+1}$, there is a path tracking algorithm which outputs $D(\lambda)$ up to accuracy $\epsilon$ in
                            $O\left(\frac{1}{\epsilon}\right)$ evaluations of $f$ and its derivatives.
                            </p>
                        </div>
                        <p> The big-$O$ suppresses dependence on condition numbers related to $\mathcal{L}$.</p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> Unbalanced Procrustes Problems</h4>
                        <p class=alignLeft> Given an $n$-dimensional point cloud $A$, and an $m$ dimensional point cloud $B$ with $n > m$, find a rotation/projection that best maps $A$ to $B$. </p>
                        <p>
                        $$ \min_{X^{\intercal}X = I, X \in \R^{n \times m}} \|AX - B\|^2.$$
                        </p>
                        <div style="display:flex">
                            <div style="padding-right:50px">
                            <video controls autoplay height=300px><source src="rotating_bunny.mp4" type="video/mp4"></video>
                            <p>A 3D model with different rotations/projections </p>
                            </div>
                            <div>
                            <img height=250px src="noisy_proj_background.png"/>
                            <p> A fixed noisy projection </p>
                            </div>
                        </div>

                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> Unbalanced Procrustes Problems</h4>
                        <p class=alignLeft> Artificially view this as a Lagrangian problem by introducing $f:\St^{n,m} \rightarrow \R^2$ with
                        \[f(X) = (\langle A^{\intercal}B, X\rangle, \|AX - B\|^2).\]
                        </p>
                        <p class="alignLeft">
                        The Lagrangian optimization problems then look like 
                        \[
                        \underset{X\in \St^{n,m}}{\argmax}\; \lambda_0 \langle A^{\intercal}B, X\rangle + \lambda_1 \|AX - B\|^2.
                        \]
                        when $\lambda_1 = 0$, this is easy to solve. When $\lambda_0 = 0$, this is the UPP.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> Unbalanced Procrustes Problems</h4>
                        <div style="background-color:lightgray;padding:30px;">
                            <h4 style="text-align:left;"> Theorem (CDR<span style="color:var(--pastel-blue)">S</span> 2025) </h4>
                            <p>
                                If $A \in \R^{d \times 3}$ and $B \in \R^{d \times 2}$ satisfy certain explicit inequalities, then the function 
                                \[
                                    f(X) = (\langle A^{\intercal}B, X\rangle, \|AX - B\|^2).
                                \]
                                has a Lagrangian dual section.
                            </p>
                        </div>
                        <p> Roughly 95% of uniformly randomly chosen $A$ and $B$ satisfy these inequalities. </p>
                    </section>
                    <section data-auto-animate>
                        <h4 class=alignLeft> Unbalanced Procrustes Problems</h4>
                        <img src="bunny_example.png"/>
                    </section>
                    <section data-auto-animate>
                        <h4> Summary</h4>
                        <ul class=alignLeft>
                            <li> Topological properties of the Lagrangian imply convexity </li>
                            <li> Globally optimal path tracking algorithms </li>
                        </ul>
                    </section>
                </section>
                    <section data-auto-animate>
                        <h4> Outline of the Talk </h4>
                        <ol>
                            <li> Introduction </li>
                            <br>
                            <li> A Topological Framework for Hidden Convexity </li>
                            <ul>
                                <li> Lagrangian Dual Sections </li>
                                <li> Path Tracking Algorithms </li>
                            </ul>
                            <br>
                            <li> <b>Automatic Design of First-Order Methods</b></li>
                            <ul>
                                <li> Long Step Gradient Descent</li>
                                <li> Subgame Perfect Gradient Method</li>
                            </ul>
                            <br>
                            <li> Conclusions and Future Work</li>
                        </ol>
                    </section>
                <section>
                    <section data-auto-animate>
                        <h3> Automatic Design of Algorithms<sup>1, 2, 3</sup>  </h3>

                        <p style="font-size:30px; margin-top:200px; text-align:left">1. <i>Accelerated objective gap and gradient norm convergence for gradient descent via long steps (INFORMS JOC, 2024)</i></p>
                        <p style="font-size:30px; text-align:left">2. <i>Composing optimized stepsize schedules for gradient descent</i>(MOR, 2025)</p>
                        <p style="font-size:30px; text-align:left">3. <i>Beyond Minimax Optimality: A Subgame Perfect Gradient Method (In revision at Math Prog, 2025)</i></p>
                        <p><b>All joint with Alex Wang and Ben Grimmer</b></p>
                    </section>
                    <section>
                        <h4> First-Order Methods</h4>
                        <p> <b>Goal:</b> Minimize functions $f : \R^d \rightarrow \R$. </p>
                        <p class=fragment> <b> Input: </b> A black box that outputs the values $f(x)$ and $\nabla f(x)$ at points of our choosing. </p>
                        <p class=fragment> <b> First-order methods:</b> Algorithm for choosing query points $x_0, \dots, x_N$. </p>
                        <p class=fragment> We will be focused on analyzing performance on <b> convex functions with $L$-Lipschitz gradients ($L$-smooth convex functions).</b></p>
                        <img src="fom.png" height=200px/>
                    </section>
                    <section data-auto-animate>
                        <h3> First Order Methods </h3>
                        <h4> Gradient Descent  </h4>
                    </section>
                    <section data-auto-animate>
                        <h4> Accelerating Gradient Descent </h4>
                        <p>Gradient descent sets \[x_i = x_{i-1} - h_i\nabla f(x_{i-1}).\] </p>
                        <p class=fragment>Standard step size choice is $h_i = \frac{1}{L}$, converging at a rate of $O(1/n)$.</p>
                        <div class=fragment>
                        <p> Much work treats constant step size case, all converging at $O(1/n)$ rate.</p>
                        <ul>
                            <li> Y. Drori and M. Teboulle. (2014)</li>
                            <li> T. Rotaru, F. Glineur, and P. Patrinos. (2024).</li>
                            <li> J. Kim. (2024)</li>
                        </ul>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4>Our step sizes</h4>
                        <p> Can gradient descent be asymptotically faster on convex functions just by tuning the step sizes? </p>
                        <p class=fragment>  <span class="fragment" style="color:red">Yes! With nonmonotonic, unbounded size, and typically asymmetric step sizes. </span></p>
                        <p class=fragment> First work: (AP 2024) and (G<span style="color:var(--pastel-blue)">S</span>W 2024).</p>
                    </section>
                    <section data-auto-animate>
                        <h4>Our step sizes</h4>

                        <p> Our latest result: </p>
                        <div style="display:flex" class=fragment>
                            <div style="flex:1">

                                <div style="background-color:lightgray;padding:30px;">
                                    <h4 style="text-align:left;"> Theorem (G<span style="color:var(--pastel-blue)">S</span>W 2024) </h4>
                                    <p>
                                        There is a dynamic program for finding an optimized<sup>*</sup> sequence of a sequence of $n$ step sizes achieving a worst case rate of convergence of $O(1/n^{1.27})$. 
                                    </p>
                                </div>
                            </div>
                            <div style="flex:1">
                                <img src="n50.png">
                            </div>
                        </div>
                        <p class=fragment> <sup>*</sup>conjectured to be <b>minimax optimal</b> amongst gradient descent methods. </p>
                        <p class=fragment> Simultaneous with Jiang and Zhang. </p>
                    </section>
                    <section>
                        <h4>Our step sizes - Intuition</h4>
                        <p> Two extreme functions: </p>
                        <div style="display:flex">
                            <div style="flex:1">
                                <img  height=200px src="quadratic.png"/>
                                <p style="text-align:center; font-size:20px">$f(x) = \frac{L}{2}x^2$</p>
                            </div>
                            <div style="flex:1">
                                <img height=200px src="huber.png"/>
                                <p style="text-align:center; font-size:20px">$f(x) = \begin{cases} L\left(|x|-\frac{1}{2}\right) \text{ if }|x| > 1\\ \frac{L}{2}x^2 \text{ otherwise}\end{cases}$</p>
                            </div>
                        </div>
                        <p> Optimal step sizes balance performance on these two functions. </p>
                        <p class=fragment> <b>How do we analyze these algorithms formally? </b></p>
                    </section>
                    <section data-auto-animate>
                        <h3> Analysis of Algorithms via Convex Optimization  </h3>
                    </section>
                    <section>
                        <h4> Performance Estimation Problems </h4>
                        <p> For worst case analysis, need to answer a <b>dual question</b>.</p>
                        </p>
                        <div style="padding:20px; background-color:black">
                        <p style="color:white"> <b> For which function $f$ does the algorithm achieve its worst performance? </b></p>
                        </div>
                        <p class="fragment"> This can be solved over $L$-smooth convex functions with convex optimization! (DT 2012).</p>
                    </section>
                    <section>
                        <h4> Performance Estimation Problems </h4>
                        <p> Formally, define error metric </p>
                        <p>

                        \[
                            \SUBOPT(\text{Alg},f) = \frac{f(x_N) - f_{min}}{\|x_0 - x_{\star}\|^2}?
                        \]
                        </p>
                        <p> And optimization problem </p>
                        <table>
                            <th></th>
                            <tr style="border-bottom:none;"> <td> $\max_f$ </td> <td> $\SUBOPT(\text{Alg},f)$ </td> </tr>
                            <tr> <td> s.t. </td> <td> $f$ is $L$-smooth and convex. </td> </tr>
                        </table>
                        <p class=fragment>Solve this by <b>reformulating</b> as a convex program via <b>interpolation conditions</b> (THG 2017).</p>
                        <p class=fragment> We give explicit dual solutions to this convex program to prove the convergence rates of our step size sequences.</p>
                    </section>
                    <section>
                        <h4> Minimax Optimality </h4>
                        <p> A first-order algorithm is minimax optimal for some class of algorithms $\mathcal{A}$ if its worst case suboptimality is minimized in that class, i.e. equal to </p>
                        <table>
                            <th></th>
                            <tr style="border-bottom:none;"> <td> <p>$\min_{\text{Alg} \in \mathcal{A}}$</p></td>
                                    <td> <p>$\max_f$</p> </td> <td> <p>$\SUBOPT(\text{Alg}, f)$</p> </td> </tr>
                            <tr> <td></td> <td> <p>s.t.</p> </td> <td> <p>$f$ is $L$-smooth and convex.</p> </td> </tr>
                        </table>
                        <p class=fragment> We conjecture the step sizes discovered in (G<span style="color:var(--pastel-blue);">S</span>W 2025) are minimax optimal amongst all gradient descent methods. </p>
                        <p class=fragment> Earlier work in (KF 2016) are minimax optimal amongst all first-order methods. </p>
                        <p class=fragment> Is minimax optimality the best guarantee a first-order method can provide? </p>
                    </section>
                    <section  data-auto-animate>
                        <h4> Subgame Perfect </h4>
                        <div style="background-color:lightgray; padding:30px;">
                            <h4 style="text-align:left;"> Subgame Perfect </h4>
                            <p class=alignLeft>
                            An algorithm is <b>subgame perfect</b> if <span class=fragment>for <b>any</b> first order history $\mathcal{H} = (x_i, f(x_i), \nabla f(x_i))_{i=0}^n$, it guarantees $\SUBOPT \le \tau$, where</span>
                                <table class=fragment>
                                    <tr style="border-bottom:none;"> <td> <p>$\tau(\mathcal{H}) = \min_{\text{Alg} \in A(\mathcal{H})}$</p></td>
                                        <td> <p>$\max_{f \in F(\mathcal{H})}$</p> </td> <td> <p>$\SUBOPT(\text{Alg}, f)$</p> </td> </tr>
                                    <tr> <td></td> <td> <p>s.t.</p> </td> <td> <p>$f$ is $L$-smooth and convex.</p> </td> </tr>
                                </table>
                        </div>
                        <div class=fragment>
                        <p> Here, $A(\mathcal{H})$ are the <b> algorithms consistent with $\mathcal{H}$</b></p>
                        <p> $F(\mathcal{H})$ are the <b> $L$-smooth convex functions consistent with $\mathcal{H}$</b></p>
                        </div>
                    </section>
                    <section>
                        <h4> Subgame Perfect Gradient Method </h4>
                        <div style="background-color:lightgray; padding:30px;">
                            <h4 style="text-align:left;"> Theorem (G<span style="color:var(--pastel-blue)">S</span>W 2025)</h4>
                            <p class=alignLeft> There is a subgame perfect gradient method that runs in polynomial time per iteration. </p>
                        </div>
                    </section>
                    <section>
                        <h4> Subgame Perfect Gradient Method </h4>
                        <b> Key idea</b>
                        <p> In each iteration, solve the worst-case function optimization problem. </p>
                        <table>
                            <th></th>
                            <tr style="border-bottom:none;"> <td> max </td> <td> $\SUBOPT'$ </td> </tr>
                            <tr> <td> s.t. </td> <td> $f$ is $L$-smooth and convex. </td> </tr>
                            <tr> <td> For each $i < n$, </td> <td> $f(x_i)$, $\nabla f(x_i)$ are fixed. </td> </tr>
                        </table>
                        <p> $\SUBOPT'$ is a slight modification of $\SUBOPT$. </p>
                        <p class=fragment> This can be reformulated as a second order cone program and solved in $O(n^{3.5} + dn)$ time, where $n$ is the number of iterations.</p>
                    </section>
                    <section>
                        <h4> Subgame Perfect Gradient Method </h4>
                        <img src="instance.png"/>
                    </section>
                    <section>
                        <h4> Summary</h4>
                        <p> Convex optimization can be used to design first-order algorithms (for convex optimization) </p>
                        <p> Game theoretic perspective leads to faster convergence. </p>
                    </section>
                </section>
                <section>
                    <section>
                        <h4> Conclusions and Future Work</h4>
                    </section>
                    <section>
                        <h4> Topology and Optimization </h4>
                        <p> Deepen connections between topology and optimization. </p>
                        <ul> 
                            <li> <b>Approximate convexity</b> - Does <b>approximate</b> continuity for Lagrangian maximizers imply <b> approximate </b> convexity (small integrality gaps)?</li>
                            <li style="margin-left:30px"> <p> Quantitative homotopy theory </p></li>
                        </ul>
                    </section>
                    <section>
                        
                        <h4> Projection Simplicity </h4>
                        <p> <b>Lifting </b> approaches are central to convex optimization. <b>Extension complexity</b> quantifies this. </p>
                        <p> <b>Projection simplicity</b> - can we understand when low dimensional projections are simpler to optimize over?  </p>
                        <ul> 
                            <li> In polytope case, interesting examples coming from zonotopes. Possible connections to matroid polytopes? </li>
                        </ul>
                    </section>
                    <section>
                        <h4> Algorithm Design via Convex Relaxations </h4>
                        <p> Designing algorithms requires proving that there are <b> no instances where the algorithm fails</b>. </p>
                        <p> Convex relaxations bounded the possible bad instances for first order algorithms. </p>
                        <ul> 
                            <li> Can we apply this methodology to <b>combinatorial algorithms</b> e.g. sorting networks? </li>
                        </ul>
                    </section>
                    <section>
                        <h3> References </h3>
                        <p> Topology </p>
                        <ul>
                            <li style="font-size:20px; text-align:left; margin-left:-60px"> <i>Hidden convexity, optimization, and algorithms on rotation matrices</i> - A Ramachandran, <span style="color:var(--pastel-blue)">K Shu</span>, AL Wang (MOR, 2024)</li>
                            <li style="font-size:20px; text-align:left; margin-left:-60px"> <i>Lagrangian Dual Sections: A Topological View of Hidden Convexity</i> - V Chandrasekaran, T Duff, J Rodriguez, <span style="color:var(--pastel-blue)">K Shu</span> (In Submission, 2025)</li>
                        </ul>
                        <p> First-Order Methods </p>
                        <ul>
                            <li style="font-size:20px; text-align:left; margin-left:-60px"> <i>Accelerated objective gap and gradient norm convergence for gradient descent via long steps</i> - B Grimmer, <span style="color:var(--pastel-blue)">K Shu</span>, AL Wang (INFORMS JOC, 2024)</li>
                            <li style="font-size:20px; text-align:left; margin-left:-60px"> <i>Composing optimized stepsize schedules for gradient descent</i> - B Grimmer, <span style="color:var(--pastel-blue)">K Shu</span>, AL Wang (MOR, 2025)</li>
                            <li style="font-size:20px; text-align:left; margin-left:-60px"> <i>Beyond Minimax Optimality: A Subgame Perfect Gradient Method</i> - B Grimmer, <span style="color:var(--pastel-blue)">K Shu</span>, AL Wang (In Revision - Math Prog, 2025)</li>
                        </ul>
                        <p> Hyperbolic Polynomials </p>
                        <ul>
                            <li style="font-size:20px; text-align:left; margin-left:-60px"> <i> Hyperbolic Relaxation of k-Locally Positive Semidefinite Matrices</i> - G Blekherman, S Dey, <span style="color:var(--pastel-blue)">K Shu</span>, S Sun (SIOPT, 2022)</li>
                            <li style="font-size:20px; text-align:left; margin-left:-60px"> <i> Linear Principal Minor Polynomials: Hyperbolic Determinantal Inequalities and Spectral Containment</i> - G Blekherman, M Kummer, R Sanyal, <span style="color:var(--pastel-blue)">K Shu</span>, S Sun (IMRN, 2022)</li>
                            <li style="font-size:20px; text-align:left; margin-left:-60px"> <i> Symmetric Hyperbolic Polynomials</i> - G Blekherman, J Lindberg, <span style="color:var(--pastel-blue)">K Shu</span> (J Pure and Appl. Algebra, 2025)</li>
                        </ul>
                        <p> Probability </p>
                        <ul>
                            <li style="font-size:20px; text-align:left; margin-left:-60px"> <i>  Debiasing Polynomial and Fourier Regression </i> - C Camano, R Meyer, <span style="color:var(--pastel-blue)">K Shu</span>, S Sun (SOSA, 2025)</li>
                            <li style="font-size:20px; text-align:left; margin-left:-60px"> <i>  A Semidefinite Hierarchy for the Expected Independence Number of a Random Graph </i> - D Cifuentes, <span style="color:var(--pastel-blue)">K Shu</span>, A Toriello (Opt Letters, 2025)</li>
                        </ul>
                    </section>
                    <section>
                        <h4> Conclusions </h4>
                        <p> <b>Focuses of my work: </b></p>
                        <p> Applications of convex optimizations</p>
                        <ul>
                            <li> Manifold Optimization </li>
                            <li> Algorithm Design </li>
                        </ul>
                        <p> Mathematical Connections to </p>
                        <ul>
                            <li> Topology </li>
                            <li> Game Theory </li>
                        </ul>
                    </section>
                </section>
            </div>

		</div>

		<script src="reveal.js"></script>
		<script src="math.js"></script>
		<script>
			Reveal.initialize({
				history: true,
				transition: 'linear',
                slideNumber: true,

				mathjax2: {
					config: 'TeX-AMS_HTML-full',
					TeX: {
						Macros: {
							R: '\\mathbb{R}',
							SO: '\\text{SO}',
							Gr: '\\text{Gr}',
							St: '\\text{St}',
							tr: '\\text{tr}',
							E: '\\mathbb{E}',
							conv: '\\text{conv}',
							L: '\\mathcal{L}',
							SUBOPT: '\\text{SUBOPT}',
							argmax: '\\text{argmax}',
							set: [ '\\left\\{#1 \\right\\}', 2 ]
						}
					}
				},

				// There are three typesetters available
				// RevealMath.MathJax2 (default)
				// RevealMath.MathJax3
				// RevealMath.KaTeX
				//
				// More info at https://revealjs.com/math/
				plugins: [ RevealMath.MathJax2 ]
			});
            Reveal.addEventListener( 'drawCurve', function() {
                element = document.getElementById("curveDrawing");
                element.classList.add("path");
            } );
		</script>

	</body>
</html>
